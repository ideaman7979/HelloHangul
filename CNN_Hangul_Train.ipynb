{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(serialized_example):\n",
    "    features = tf.parse_single_example(\n",
    "      serialized_example,      \n",
    "      features={\n",
    "          'label': tf.FixedLenFeature([], tf.int64),\n",
    "          'width': tf.FixedLenFeature([], tf.int64),\n",
    "          'height': tf.FixedLenFeature([], tf.int64),\n",
    "          'image': tf.FixedLenFeature([], tf.string)\n",
    "      })\n",
    "    image = tf.decode_raw(features['image'],tf.uint8)\n",
    "    #image = tf.reshape(image,[64,64,1])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [64,64,1])\n",
    "    label = tf.cast(features['label'],tf.int64)\n",
    "    \n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "train_file_name = \"Shuffled_PE92_Train.tfrecord\"\n",
    "train_dataset = tf.data.TFRecordDataset(\"./\"+train_file_name)\n",
    "train_dataset = train_dataset.map(decode)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset= train_dataset.shuffle(buffer_size=10000)\n",
    "train_dataset = train_dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = \"Shuffled_PE92_Test.tfrecord\"\n",
    "test_dataset = tf.data.TFRecordDataset(\"./\"+test_file_name)\n",
    "test_dataset = test_dataset.map(decode)\n",
    "test_dataset = test_dataset.repeat()\n",
    "test_dataset= test_dataset.shuffle(buffer_size=100)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200215\n"
     ]
    }
   ],
   "source": [
    "train_data_length = sum(1 for _ in tf.python_io.tf_record_iterator(\"./\"+train_file_name))\n",
    "print(train_data_length)\n",
    "\n",
    "test_data_length = sum(1 for _ in tf.python_io.tf_record_iterator(\"./\"+test_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "test_iterator = tf.data.Iterator.from_structure(test_dataset.output_types,\n",
    "                                           test_dataset.output_shapes)\n",
    "\n",
    "train_image, train_label = train_iterator.get_next()\n",
    "test_image, test_label = train_iterator.get_next()\n",
    "\n",
    "train_init_op = train_iterator.make_initializer(train_dataset)\n",
    "test_init_op = test_iterator.make_initializer(test_dataset)\n",
    "\n",
    "train_total_batch = int(train_data_length/batch_size)\n",
    "test_total_batch = int(test_data_length/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 64, 64, 1), dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "input_images = tf.placeholder(tf.float32,[None,64,64,1])\n",
    "input_labels = tf.placeholder(tf.int64,[None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "print(input_images)\n",
    "print(input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D:0\", shape=(?, 64, 64, 32), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 32, 32, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W1 = tf.Variable(tf.random_normal([5,5,1,32], stddev=0.01))\n",
    "L1 = tf.nn.conv2d(input_images,W1, strides=[1,1,1,1], padding = 'SAME')\n",
    "print(L1)\n",
    "bn1 = tf.layers.batch_normalization(L1, training=is_training)\n",
    "at1 = tf.nn.relu(bn1)\n",
    "\n",
    "pool1 =  tf.nn.max_pool(at1, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "print(pool1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_1:0\", shape=(?, 16, 16, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W2 = tf.Variable(tf.random_normal([5, 5, 32, 64], stddev=0.01))\n",
    "\n",
    "L2 = tf.nn.conv2d(pool1,W2, strides=[1,1,1,1], padding = 'SAME')\n",
    "bn2 = tf.layers.batch_normalization(L2, training=is_training)\n",
    "at2 = tf.nn.relu(bn2)\n",
    "\n",
    "pool2 =  tf.nn.max_pool(at2, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "print(pool2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_2:0\", shape=(?, 8, 8, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W3 = tf.Variable(tf.random_normal([4, 4, 64, 128], stddev=0.01))\n",
    "\n",
    "L3 = tf.nn.conv2d(pool2,W3, strides=[1,1,1,1], padding = 'SAME')\n",
    "bn3 = tf.layers.batch_normalization(L3, training=is_training)\n",
    "at3 = tf.nn.relu(bn3)\n",
    "\n",
    "pool3 =  tf.nn.max_pool(at3, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "print(pool3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_3:0\", shape=(?, 4, 4, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W4 = tf.Variable(tf.random_normal([4, 4, 128, 256], stddev=0.01))\n",
    "\n",
    "L4 = tf.nn.conv2d(pool3,W4, strides=[1,1,1,1], padding = 'SAME')\n",
    "bn4 = tf.layers.batch_normalization(L4, training=is_training)\n",
    "at4 = tf.nn.relu(bn4)\n",
    "\n",
    "pool4 =  tf.nn.max_pool(at4, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "print(pool4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dropout/mul:0\", shape=(?, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W5 = tf.Variable(tf.random_normal([4*4*256, 1024], stddev=0.01))\n",
    "fc1 = tf.reshape(pool4,[-1,4*4*256])\n",
    "fc1 = tf.matmul(fc1, W5)\n",
    "fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "fc1 = tf.nn.dropout(fc1,keep_prob)\n",
    "print(fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_1:0\", shape=(?, 2350), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W6 = tf.Variable(tf.random_normal([1024,2350],stddev=0.01))\n",
    "logit = tf.matmul(fc1, W6)\n",
    "print(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=input_labels))\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logit,1),input_labels)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train total Batch :2002\n",
      "Epoch :0 | step 0, training accuracy: 0.020000\n",
      "Epoch :0 | step 100, training accuracy: 0.000000\n",
      "Epoch :0 | step 200, training accuracy: 0.000000\n",
      "Epoch :0 | step 300, training accuracy: 0.000000\n",
      "Epoch :0 | step 400, training accuracy: 0.000000\n",
      "Epoch :0 | step 500, training accuracy: 0.000000\n",
      "Epoch :0 | step 600, training accuracy: 0.000000\n",
      "Epoch :0 | step 700, training accuracy: 0.000000\n",
      "Epoch :0 | step 800, training accuracy: 0.000000\n",
      "Epoch :0 | step 900, training accuracy: 0.010000\n",
      "Epoch :0 | step 1000, training accuracy: 0.000000\n",
      "Epoch :0 | step 1100, training accuracy: 0.010000\n",
      "Epoch :0 | step 1200, training accuracy: 0.000000\n",
      "Epoch :0 | step 1300, training accuracy: 0.000000\n",
      "Epoch :0 | step 1400, training accuracy: 0.000000\n",
      "Epoch :0 | step 1500, training accuracy: 0.000000\n",
      "Epoch :0 | step 1600, training accuracy: 0.000000\n",
      "Epoch :0 | step 1700, training accuracy: 0.000000\n",
      "Epoch :0 | step 1800, training accuracy: 0.000000\n",
      "Epoch :0 | step 1900, training accuracy: 0.000000\n",
      "Epoch :0 | step 2000, training accuracy: 0.000000\n",
      "Test Acc : 0.000000\n",
      "Epoch :1 | step 0, training accuracy: 0.020000\n",
      "Epoch :1 | step 100, training accuracy: 0.000000\n",
      "Epoch :1 | step 200, training accuracy: 0.010000\n",
      "Epoch :1 | step 300, training accuracy: 0.000000\n",
      "Epoch :1 | step 400, training accuracy: 0.010000\n",
      "Epoch :1 | step 500, training accuracy: 0.000000\n",
      "Epoch :1 | step 600, training accuracy: 0.010000\n",
      "Epoch :1 | step 700, training accuracy: 0.020000\n",
      "Epoch :1 | step 800, training accuracy: 0.000000\n",
      "Epoch :1 | step 900, training accuracy: 0.000000\n",
      "Epoch :1 | step 1000, training accuracy: 0.000000\n",
      "Epoch :1 | step 1100, training accuracy: 0.010000\n",
      "Epoch :1 | step 1200, training accuracy: 0.020000\n",
      "Epoch :1 | step 1300, training accuracy: 0.000000\n",
      "Epoch :1 | step 1400, training accuracy: 0.000000\n",
      "Epoch :1 | step 1500, training accuracy: 0.000000\n",
      "Epoch :1 | step 1600, training accuracy: 0.010000\n",
      "Epoch :1 | step 1700, training accuracy: 0.000000\n",
      "Epoch :1 | step 1800, training accuracy: 0.020000\n",
      "Epoch :1 | step 1900, training accuracy: 0.010000\n",
      "Epoch :1 | step 2000, training accuracy: 0.010000\n",
      "Epoch :2 | step 0, training accuracy: 0.000000\n",
      "Epoch :2 | step 100, training accuracy: 0.000000\n",
      "Epoch :2 | step 200, training accuracy: 0.000000\n",
      "Epoch :2 | step 300, training accuracy: 0.010000\n",
      "Epoch :2 | step 400, training accuracy: 0.010000\n",
      "Epoch :2 | step 500, training accuracy: 0.000000\n",
      "Epoch :2 | step 600, training accuracy: 0.010000\n",
      "Epoch :2 | step 700, training accuracy: 0.020000\n",
      "Epoch :2 | step 800, training accuracy: 0.000000\n",
      "Epoch :2 | step 900, training accuracy: 0.010000\n",
      "Epoch :2 | step 1000, training accuracy: 0.010000\n",
      "Epoch :2 | step 1100, training accuracy: 0.020000\n",
      "Epoch :2 | step 1200, training accuracy: 0.020000\n",
      "Epoch :2 | step 1300, training accuracy: 0.030000\n",
      "Epoch :2 | step 1400, training accuracy: 0.000000\n",
      "Epoch :2 | step 1500, training accuracy: 0.020000\n",
      "Epoch :2 | step 1600, training accuracy: 0.010000\n",
      "Epoch :2 | step 1700, training accuracy: 0.020000\n",
      "Epoch :2 | step 1800, training accuracy: 0.010000\n",
      "Epoch :2 | step 1900, training accuracy: 0.000000\n",
      "Epoch :2 | step 2000, training accuracy: 0.010000\n",
      "Epoch :3 | step 0, training accuracy: 0.030000\n",
      "Epoch :3 | step 100, training accuracy: 0.020000\n",
      "Epoch :3 | step 200, training accuracy: 0.000000\n",
      "Epoch :3 | step 300, training accuracy: 0.000000\n",
      "Epoch :3 | step 400, training accuracy: 0.010000\n",
      "Epoch :3 | step 500, training accuracy: 0.020000\n",
      "Epoch :3 | step 600, training accuracy: 0.000000\n",
      "Epoch :3 | step 700, training accuracy: 0.010000\n",
      "Epoch :3 | step 800, training accuracy: 0.030000\n",
      "Epoch :3 | step 900, training accuracy: 0.020000\n",
      "Epoch :3 | step 1000, training accuracy: 0.000000\n",
      "Epoch :3 | step 1100, training accuracy: 0.000000\n",
      "Epoch :3 | step 1200, training accuracy: 0.020000\n",
      "Epoch :3 | step 1300, training accuracy: 0.000000\n",
      "Epoch :3 | step 1400, training accuracy: 0.030000\n",
      "Epoch :3 | step 1500, training accuracy: 0.030000\n",
      "Epoch :3 | step 1600, training accuracy: 0.030000\n",
      "Epoch :3 | step 1700, training accuracy: 0.020000\n",
      "Epoch :3 | step 1800, training accuracy: 0.040000\n"
     ]
    }
   ],
   "source": [
    "init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print(\"Train total Batch :%d\" % (train_total_batch))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    coord=tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    #Train Model\n",
    "    loss_sum=0\n",
    "    start_time = time.time()\n",
    "    for epoch in range(50):\n",
    "        sess.run(train_init_op)\n",
    "        for i in range(train_total_batch):\n",
    "        #for i in range(batch_size):\n",
    "            batch_images, batch_labels = sess.run([train_image, train_label])\n",
    "            sess.run(optimizer,feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:0.5, is_training:True})\n",
    "            train_acc =sess.run(accuracy, feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:1.0,is_training:False})\n",
    "            \n",
    "            if(i%100==0):\n",
    "                print('Epoch :%d | step %d, training accuracy: %f' % (epoch,i, train_acc))\n",
    "\n",
    "        save_path = saver.save(sess, \"save\\model_PE92.ckpt\")\n",
    "        if(epoch%5==0):\n",
    "            sess.run(test_init_op)#Test iterator 생성\n",
    "            batch_images, batch_labels = sess.run([test_image, test_label])\n",
    "            test_acc, train_loss=sess.run([accuracy, loss], feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:1.0, is_training:False})\n",
    "            print(\"Test Acc : %f\" %(test_acc))\n",
    "            \n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    \n",
    "    print(\"Training is done.\")\n",
    "    print(\"Accuracy :%f\" %(train_acc))\n",
    "    print(\"Time : %f\" %(elapsed))\n",
    "    \n",
    "    #Test Model\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    coord=tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    sess.run(train_init_op)\n",
    "    start_time = time.time()\n",
    "    for i in range(train_total_batch):\n",
    "        #for i in range(batch_size):\n",
    "        batch_images, batch_labels = sess.run([train_image, train_label])\n",
    "        print(batch_labels)\n",
    "\n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Tensor(\"random_uniform:0\", shape=(8,), dtype=float32)\n",
      "Tensor(\"one_hot:0\", shape=(8,), dtype=float32)\n",
      "1.1221625 1.1221625\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from random import randint\n",
    "\n",
    "dims = 8\n",
    "pos  = randint(0, dims - 1)\n",
    "print(pos)\n",
    "logits = tf.random_uniform([dims], maxval=3, dtype=tf.float32)\n",
    "labels = tf.one_hot(pos, dims)\n",
    "print(logits)\n",
    "print(labels)\n",
    "\n",
    "res1 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)\n",
    "res2 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=tf.constant(pos))\n",
    "loss=tf.reduce_mean(res2)\n",
    "with tf.Session() as sess:\n",
    "    a, b = sess.run([res1, loss])\n",
    "    print (a, b)\n",
    "    print (a == b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_decode(filename_queue, batch_size):\n",
    "    tf_record_reader=tf.TFRecordReader()\n",
    "    _, tf_record_serialized = tf_record_reader.read(tf_record_filename_queue)\n",
    "\n",
    "    tf_record_features = tf.parse_single_example(\n",
    "        tf_record_serialized,\n",
    "        features={\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.FixedLenFeature([], tf.int64),\n",
    "            'height': tf.FixedLenFeature([], tf.int64),\n",
    "            'image': tf.FixedLenFeature([], tf.string),\n",
    "        })\n",
    "    tf_record_image = tf.decode_raw(tf_record_features['image'],tf.uint8)\n",
    "    \n",
    "    tf_record_image = tf.cast(tf_record_image, tf.float32)\n",
    "\n",
    "    tf_record_image = tf.reshape(tf_record_image, [64,64,1])\n",
    "\n",
    "    tf_record_label = tf.cast(tf_record_features['label'],tf.int64)\n",
    "    \n",
    "    images, labels = tf.train.shuffle_batch([tf_record_image, tf_record_label],\n",
    "                                         batch_size = batch_size,\n",
    "                                         capacity=20*batch_size,\n",
    "                                         min_after_dequeue=5*batch_size )\n",
    "    return images, labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
