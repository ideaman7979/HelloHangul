{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(serialized_example):\n",
    "    features = tf.parse_single_example(\n",
    "      serialized_example,      \n",
    "      features={\n",
    "          'label': tf.FixedLenFeature([], tf.int64),\n",
    "          'width': tf.FixedLenFeature([], tf.int64),\n",
    "          'height': tf.FixedLenFeature([], tf.int64),\n",
    "          'image': tf.FixedLenFeature([], tf.string)\n",
    "      })\n",
    "    image = tf.decode_raw(features['image'],tf.uint8)\n",
    "    #image = tf.reshape(image,[64,64,1])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [64,64,1])\n",
    "    label = tf.cast(features['label'],tf.int64)\n",
    "    \n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200215\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "test_file_name = \"Shuffled_PE92_Test.tfrecord\"\n",
    "train_file_name = \"Shuffled_PE92_Train.tfrecord\"\n",
    "train_data_length = sum(1 for _ in tf.python_io.tf_record_iterator(\"./\"+train_file_name))\n",
    "print(train_data_length)\n",
    "\n",
    "test_data_length = sum(1 for _ in tf.python_io.tf_record_iterator(\"./\"+test_file_name))\n",
    "total_test_batch = int(test_data_length/batch_size)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data set\n",
    "test_dataset = tf.data.TFRecordDataset(\"./\"+test_file_name)\n",
    "test_dataset = test_dataset.map(decode)\n",
    "#test_dataset = test_dataset.repeat()\n",
    "#test_dataset= test_dataset.shuffle(buffer_size=10000)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Data set\n",
    "train_dataset = tf.data.TFRecordDataset(\"./\"+train_file_name)\n",
    "train_dataset = train_dataset.map(decode)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset= train_dataset.shuffle(buffer_size=10000)\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "test_iterator = tf.data.Iterator.from_structure(test_dataset.output_types,\n",
    "                                           test_dataset.output_shapes)\n",
    "\n",
    "train_init_op = train_iterator.make_initializer(train_dataset)\n",
    "test_init_op = test_iterator.make_initializer(test_dataset)\n",
    "\n",
    "train_image, train_label = train_iterator.get_next()\n",
    "test_image, test_label = test_iterator.get_next()\n",
    "\n",
    "train_total_batch = int(train_data_length/batch_size)\n",
    "test_total_batch = int(test_data_length/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 64, 64, 1), dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "input_images = tf.placeholder(tf.float32,[None,64,64,1])\n",
    "input_labels = tf.placeholder(tf.int64,[None])\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "print(input_images)\n",
    "print(input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool:0\", shape=(?, 32, 32, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W1 = tf.Variable(tf.random_normal([5,5,1,32], stddev=0.01))\n",
    "L1 = tf.nn.conv2d(input_images,W1, strides=[1,1,1,1], padding = 'SAME')\n",
    "#at1 = tf.nn.relu(L1)\n",
    "\n",
    "B1 = tf.Variable(tf.constant(0.1,shape=[32]))\n",
    "at1 = tf.nn.relu(L1 + B1)\n",
    "\n",
    "pool1 =  tf.nn.max_pool(at1, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "print(pool1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_1:0\", shape=(?, 16, 16, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W2 = tf.Variable(tf.random_normal([5, 5, 32, 64], stddev=0.01))\n",
    "\n",
    "L2 = tf.nn.conv2d(pool1,W2, strides=[1,1,1,1], padding = 'SAME')\n",
    "#at2 = tf.nn.relu(L2)\n",
    "\n",
    "B2 = tf.Variable(tf.constant(0.1,shape=[64]))\n",
    "at2 = tf.nn.relu(L2 + B2)\n",
    "\n",
    "\n",
    "pool2 =  tf.nn.max_pool(at2, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "print(pool2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_2:0\", shape=(?, 8, 8, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W3 = tf.Variable(tf.random_normal([4, 4, 64, 128], stddev=0.01))\n",
    "B3 = tf.Variable(tf.constant(0.1,shape=[128]))\n",
    "\n",
    "L3 = tf.nn.conv2d(pool2,W3, strides=[1,1,1,1], padding = 'SAME')\n",
    "#t3 = tf.nn.relu(L3)\n",
    "at3 = tf.nn.relu(L3 + B3)\n",
    "\n",
    "pool3 =  tf.nn.max_pool(at3, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "print(pool3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_3:0\", shape=(?, 4, 4, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W4 = tf.Variable(tf.random_normal([4, 4, 128, 256], stddev=0.01))\n",
    "B3 = tf.Variable(tf.constant(0.1,shape=[256]))\n",
    "\n",
    "L4 = tf.nn.conv2d(pool3,W4, strides=[1,1,1,1], padding = 'SAME')\n",
    "#at4 = tf.nn.relu(L4)\n",
    "at4 = tf.nn.relu(L4 + B3)\n",
    "\n",
    "pool4 =  tf.nn.max_pool(at4, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "print(pool4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dropout/mul:0\", shape=(?, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W5 = tf.Variable(tf.random_normal([4*4*256, 512], stddev=0.01))\n",
    "fc1 = tf.reshape(pool4,[-1,4*4*256])\n",
    "fb1 = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "\n",
    "fc1 = tf.matmul(fc1, W5)\n",
    "#fc1 = tf.nn.relu(fc1)\n",
    "fc1 = tf.nn.relu(fc1+fb1)\n",
    "\n",
    "fc1 = tf.nn.dropout(fc1,keep_prob)\n",
    "print(fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_5:0\", shape=(?, 2350), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W6 = tf.Variable(tf.random_normal([512,2350],stddev=0.01))\n",
    "fb2 = tf.Variable(tf.constant(0.1,shape=[2350]))\n",
    "#logit = tf.matmul(fc1, W6)\n",
    "logit = tf.matmul(fc1, W6)+fb2\n",
    "print(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=input_labels))\n",
    "optimizer = tf.train.AdamOptimizer(0.005).minimize(loss)\n",
    "loss_summ=tf.summary.scalar(\"Loss\",loss)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logit,1),input_labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "test_accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n",
    "acc_summ=tf.summary.scalar(\"Accuracy\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train total Batch :2002\n",
      "Epoch :0 | step 0, training accuracy: 0.020000, train loss: 451.694092\n",
      "Epoch :0 | step 100, training accuracy: 0.000000, train loss: 7.768229\n",
      "Epoch :0 | step 200, training accuracy: 0.000000, train loss: 7.768107\n",
      "Epoch :0 | step 300, training accuracy: 0.000000, train loss: 7.772088\n",
      "Epoch :0 | step 400, training accuracy: 0.000000, train loss: 7.781970\n",
      "Epoch :0 | step 500, training accuracy: 0.000000, train loss: 7.774735\n",
      "Epoch :0 | step 600, training accuracy: 0.000000, train loss: 7.766427\n",
      "Epoch :0 | step 700, training accuracy: 0.000000, train loss: 7.775711\n",
      "Epoch :0 | step 800, training accuracy: 0.000000, train loss: 7.764658\n",
      "Epoch :0 | step 900, training accuracy: 0.000000, train loss: 7.776766\n",
      "Epoch :0 | step 1000, training accuracy: 0.000000, train loss: 7.781356\n",
      "Epoch :0 | step 1100, training accuracy: 0.000000, train loss: 7.784872\n",
      "Epoch :0 | step 1200, training accuracy: 0.000000, train loss: 7.789337\n",
      "Epoch :0 | step 1300, training accuracy: 0.000000, train loss: 7.770554\n",
      "Epoch :0 | step 1400, training accuracy: 0.000000, train loss: 7.771689\n",
      "Epoch :0 | step 1500, training accuracy: 0.000000, train loss: 7.776337\n",
      "Epoch :0 | step 1600, training accuracy: 0.000000, train loss: 7.770269\n",
      "Epoch :0 | step 1700, training accuracy: 0.000000, train loss: 7.772116\n",
      "Epoch :0 | step 1800, training accuracy: 0.000000, train loss: 7.766867\n",
      "Epoch :0 | step 1900, training accuracy: 0.000000, train loss: 7.776843\n",
      "Epoch :0 | step 2000, training accuracy: 0.000000, train loss: 7.769681\n",
      "Time : 2.368519\n",
      "Epoch :1 | step 0, training accuracy: 0.000000, train loss: 7.769821\n",
      "Epoch :1 | step 100, training accuracy: 0.000000, train loss: 7.779478\n",
      "Epoch :1 | step 200, training accuracy: 0.000000, train loss: 7.772169\n",
      "Epoch :1 | step 300, training accuracy: 0.000000, train loss: 7.780588\n",
      "Epoch :1 | step 400, training accuracy: 0.000000, train loss: 7.763382\n",
      "Epoch :1 | step 500, training accuracy: 0.000000, train loss: 7.759063\n",
      "Epoch :1 | step 600, training accuracy: 0.000000, train loss: 7.767597\n",
      "Epoch :1 | step 700, training accuracy: 0.000000, train loss: 7.777255\n",
      "Epoch :1 | step 800, training accuracy: 0.000000, train loss: 7.773536\n",
      "Epoch :1 | step 900, training accuracy: 0.000000, train loss: 7.764192\n",
      "Epoch :1 | step 1000, training accuracy: 0.000000, train loss: 7.770744\n",
      "Epoch :1 | step 1100, training accuracy: 0.000000, train loss: 7.765176\n",
      "Epoch :1 | step 1200, training accuracy: 0.000000, train loss: 7.777495\n",
      "Epoch :1 | step 1300, training accuracy: 0.000000, train loss: 7.766318\n",
      "Epoch :1 | step 1400, training accuracy: 0.000000, train loss: 7.771511\n",
      "Epoch :1 | step 1500, training accuracy: 0.000000, train loss: 7.764788\n",
      "Epoch :1 | step 1600, training accuracy: 0.000000, train loss: 7.782207\n",
      "Epoch :1 | step 1700, training accuracy: 0.000000, train loss: 7.764178\n",
      "Epoch :1 | step 1800, training accuracy: 0.000000, train loss: 7.774420\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-22377be8b2e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#for i in range(batch_size):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mbatch_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minput_images\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmerged_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minput_images\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print(\"Train total Batch :%d\" % (train_total_batch))\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(\"./logs/Basic/PE92/Train\")\n",
    "    \n",
    "    sess.run(init)\n",
    "    coord=tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    sess.run(train_init_op)\n",
    "    sess.run(test_init_op)#\n",
    "    #Train Model\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(100):\n",
    "        for i in range(train_total_batch):\n",
    "        #for i in range(batch_size):\n",
    "            batch_images, batch_labels = sess.run([train_image, train_label])\n",
    "            sess.run(optimizer,feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:0.5})\n",
    "            summary, train_acc, train_loss = sess.run([merged_summary, accuracy, loss],feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:1.0})\n",
    "            \n",
    "            train_writer.add_summary(summary,i)\n",
    "            if(i%100==0):\n",
    "                print('Epoch :%d | step %d, training accuracy: %f, train loss: %f' % (epoch,i, train_acc,train_loss))\n",
    "\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        elapsed = elapsed / 60\n",
    "        print(\"Time : %f\" %(elapsed))\n",
    "    \n",
    "    print(\"Training is done.\")\n",
    "    #Test Model\n",
    "    save_path = saver.save(sess, \"./save/PE92/Basic/Basic_model_PE92.ckpt\")\n",
    "    acc_sum=0.0\n",
    "    for epoch in range(total_test_batch):\n",
    "        batch_images, batch_labels = sess.run([test_image, test_label])\n",
    "        try:\n",
    "            test_acc, test_loss=sess.run([test_accuracy,loss], feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:1.0})\n",
    "            test_acc=test_acc/len(batch_labels)\n",
    "            print(\"Test acc is : %f\"%(test_acc))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "              print(\"Out of Data ierators\")\n",
    "        acc_sum+=test_acc\n",
    "    test_acc = acc_sum/test_data_length\n",
    "    print(\"Test Acc : %f, Test Loss :%f\" %(test_acc,test_loss))\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    train_writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = \"Shuffled_HanDB_Train.tfrecord\"\n",
    "test_file_name = \"Shuffled_HanDB_Test.tfrecord\"\n",
    "\n",
    "train_data_length = sum(1 for _ in tf.python_io.tf_record_iterator(\"./\"+train_file_name))\n",
    "test_data_length = sum(1 for _ in tf.python_io.tf_record_iterator(\"./\"+test_file_name))\n",
    "total_test_batch = int(test_data_length/batch_size)+1\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(\"./\"+train_file_name)\n",
    "train_dataset = train_dataset.map(decode)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset= train_dataset.shuffle(buffer_size=10000)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset(\"./\"+test_file_name)\n",
    "test_dataset = test_dataset.map(decode)\n",
    "test_dataset = test_dataset.repeat()\n",
    "test_dataset= test_dataset.shuffle(buffer_size=10000)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "train_iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "test_iterator = tf.data.Iterator.from_structure(test_dataset.output_types, test_dataset.output_shapes)\n",
    "\n",
    "train_init_op = train_iterator.make_initializer(train_dataset)\n",
    "test_init_op = test_iterator.make_initializer(test_dataset)\n",
    "\n",
    "train_image, train_label = train_iterator.get_next()\n",
    "test_image, test_label = test_iterator.get_next()\n",
    "\n",
    "train_total_batch = int(train_data_length/batch_size)\n",
    "test_total_batch = int(test_data_length/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print(\"Train total Batch :%d\" % (train_total_batch))\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "   \n",
    "    \n",
    "    sess.run(init)\n",
    "    coord=tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    sess.run(train_init_op)\n",
    "    sess.run(test_init_op)#Test iterator 생성\n",
    "    #Train Model\n",
    "    loss_sum=0\n",
    "    start_time = time.time()\n",
    "    for epoch in range(100):\n",
    "        for i in range(train_total_batch):\n",
    "        #for i in range(batch_size):\n",
    "            batch_images, batch_labels = sess.run([train_image, train_label])\n",
    "            sess.run(optimizer,feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:0.5})\n",
    "            summary, train_acc, loss=sess.run([merged_summary, accuracy, loss],feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:1.0})\n",
    "            train_writer.add_summary(summary,i)\n",
    "            if(i%100==0):\n",
    "                print('Epoch :%d | step %d, training accuracy: %f, Loss: %f' % (epoch,i, train_acc,train_loss))\n",
    "\n",
    "        save_path = saver.save(sess, \"./save/HanDB/Basic/Basic_model_HanDB.ckpt\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        elapsed = elapsed/60\n",
    "        print(\"Time : %f\" %(elapsed))\n",
    "        \n",
    "    print(\"Training is done.\")\n",
    "    \n",
    "    acc_sum=0.0\n",
    "    for epoch in range(total_test_batch):\n",
    "        batch_images, batch_labels = sess.run([test_image, test_label])\n",
    "        try:\n",
    "            test_acc, test_loss=sess.run([test_accuracy,loss], feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:1.0})\n",
    "            test_acc=test_acc/len(batch_labels)\n",
    "            print(\"Test acc is : %f\"%(test_acc))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "              print(\"Out of Data ierators\")\n",
    "        acc_sum+=test_acc\n",
    "    test_acc = acc_sum/test_data_length\n",
    "    print(\"Test Acc : %f, Test Loss :%f\" %(test_acc,test_loss))\n",
    "    train_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_decode(filename_queue, batch_size):\n",
    "    tf_record_reader=tf.TFRecordReader()\n",
    "    _, tf_record_serialized = tf_record_reader.read(tf_record_filename_queue)\n",
    "\n",
    "    tf_record_features = tf.parse_single_example(\n",
    "        tf_record_serialized,\n",
    "        features={\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.FixedLenFeature([], tf.int64),\n",
    "            'height': tf.FixedLenFeature([], tf.int64),\n",
    "            'image': tf.FixedLenFeature([], tf.string),\n",
    "        })\n",
    "    tf_record_image = tf.decode_raw(tf_record_features['image'],tf.uint8)\n",
    "    \n",
    "    tf_record_image = tf.cast(tf_record_image, tf.float32)\n",
    "\n",
    "    tf_record_image = tf.reshape(tf_record_image, [64,64,1])\n",
    "\n",
    "    tf_record_label = tf.cast(tf_record_features['label'],tf.int64)\n",
    "    \n",
    "    images, labels = tf.train.shuffle_batch([tf_record_image, tf_record_label],\n",
    "                                         batch_size = batch_size,\n",
    "                                         capacity=20*batch_size,\n",
    "                                         min_after_dequeue=5*batch_size )\n",
    "    return images, labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
