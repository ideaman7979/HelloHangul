{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow  as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(serialized_example):\n",
    "    alphabets=list()\n",
    "    features = tf.parse_single_example(\n",
    "      serialized_example,      \n",
    "      features={\n",
    "          'label': tf.FixedLenFeature([], tf.int64),\n",
    "          'cho_label':tf.FixedLenFeature([], tf.int64),\n",
    "          'jung_label':tf.FixedLenFeature([], tf.int64),\n",
    "          'jong_label':tf.FixedLenFeature([], tf.int64),\n",
    "          'width': tf.FixedLenFeature([], tf.int64),\n",
    "          'height': tf.FixedLenFeature([], tf.int64),\n",
    "          'image': tf.FixedLenFeature([], tf.string)\n",
    "      })\n",
    "    image = tf.decode_raw(features['image'],tf.uint8)\n",
    "    #image = tf.reshape(image,[64,64,1])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [64,64,1])\n",
    "    label = tf.cast(features['label'],tf.int64)\n",
    "    cho_label=tf.cast(features['cho_label'],tf.int64)\n",
    "    jung_label=tf.cast(features['jung_label'],tf.int64)\n",
    "    jong_label=tf.cast(features['jong_label'],tf.int64)\n",
    "    \n",
    "    return image,label, cho_label, jung_label, jong_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "test_file_name = \"Shuffled_with_alphabet_PE92_Test.tfrecord\"\n",
    "train_file_name = \"Shuffled_with_alphabet_PE92_Train.tfrecord\"\n",
    "\n",
    "train_data_length = sum(1 for _ in tf.python_io.tf_record_iterator(\"./\"+train_file_name))\n",
    "test_data_length = sum(1 for _ in tf.python_io.tf_record_iterator(\"./\"+test_file_name))\n",
    "total_test_batch = int(test_data_length/batch_size)+1\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset(\"./\"+test_file_name)\n",
    "test_dataset = test_dataset.map(decode)\n",
    "#test_dataset = test_dataset.repeat()\n",
    "#test_dataset= test_dataset.shuffle(buffer_size=10000)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(\"./\"+train_file_name)\n",
    "train_dataset = train_dataset.map(decode)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset= train_dataset.shuffle(buffer_size=10000)\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "test_iterator = tf.data.Iterator.from_structure(test_dataset.output_types,\n",
    "                                           test_dataset.output_shapes)\n",
    "\n",
    "train_init_op = train_iterator.make_initializer(train_dataset)\n",
    "test_init_op = test_iterator.make_initializer(test_dataset)\n",
    "\n",
    "train_image, train_label, train_cho, train_jung, train_jong = train_iterator.get_next()\n",
    "test_image, test_label, test_cho, test_jung, test_jong = test_iterator.get_next()\n",
    "\n",
    "train_total_batch = int(train_data_length/batch_size)\n",
    "test_total_batch = int(test_data_length/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 64, 64, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input_images = tf.placeholder(tf.float32,[None,64,64,1])\n",
    "input_label_cho = tf.placeholder(tf.int64,[None])\n",
    "input_label_jung = tf.placeholder(tf.int64,[None])\n",
    "input_label_jong = tf.placeholder(tf.int64,[None])\n",
    "input_labels= tf.placeholder(tf.int64,[None])\n",
    "\n",
    "\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "print(input_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D:0\", shape=(?, 64, 64, 32), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 32, 32, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W1 = tf.Variable(tf.random_normal([5,5,1,32], stddev=0.01))\n",
    "L1 = tf.nn.conv2d(input_images,W1, strides=[1,1,1,1], padding = 'SAME')\n",
    "print(L1)\n",
    "bn1 = tf.layers.batch_normalization(L1, training=is_training)\n",
    "at1 = tf.nn.relu(bn1)\n",
    "\n",
    "pool1 =  tf.nn.max_pool(at1, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "print(pool1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_1:0\", shape=(?, 16, 16, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W2 = tf.Variable(tf.random_normal([5, 5, 32, 64], stddev=0.01))\n",
    "\n",
    "L2 = tf.nn.conv2d(pool1,W2, strides=[1,1,1,1], padding = 'SAME')\n",
    "bn2 = tf.layers.batch_normalization(L2, training=is_training)\n",
    "at2 = tf.nn.relu(bn2)\n",
    "\n",
    "pool2 =  tf.nn.max_pool(at2, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "print(pool2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_2:0\", shape=(?, 8, 8, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W3 = tf.Variable(tf.random_normal([4, 4, 64, 128], stddev=0.01))\n",
    "\n",
    "L3 = tf.nn.conv2d(pool2,W3, strides=[1,1,1,1], padding = 'SAME')\n",
    "bn3 = tf.layers.batch_normalization(L3, training=is_training)\n",
    "at3 = tf.nn.relu(bn3)\n",
    "\n",
    "pool3 =  tf.nn.max_pool(at3, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "print(pool3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_3:0\", shape=(?, 4, 4, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W4 = tf.Variable(tf.random_normal([4, 4, 128, 256], stddev=0.01))\n",
    "\n",
    "L4 = tf.nn.conv2d(pool3,W4, strides=[1,1,1,1], padding = 'SAME')\n",
    "bn4 = tf.layers.batch_normalization(L4, training=is_training)\n",
    "at4 = tf.nn.relu(bn4)\n",
    "\n",
    "pool4 =  tf.nn.max_pool(at4, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "print(pool4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(?, 4096), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W_cho1 = tf.Variable(tf.random_normal([4*4*256, 256], stddev=0.01))\n",
    "W_jung1 = tf.Variable(tf.random_normal([4*4*256, 256], stddev=0.01))\n",
    "W_jong1 = tf.Variable(tf.random_normal([4*4*256, 256], stddev=0.01))\n",
    "fc = tf.reshape(pool4,[-1,4*4*256])\n",
    "\n",
    "fc_cho = tf.matmul(fc, W_cho1)\n",
    "fc_cho = tf.layers.batch_normalization(fc_cho, training=is_training)\n",
    "fc_cho = tf.nn.relu(fc_cho)\n",
    "fc_cho= tf.nn.dropout(fc_cho,keep_prob)\n",
    "\n",
    "fc_jung = tf.matmul(fc, W_jung1)\n",
    "fc_jung = tf.layers.batch_normalization(fc_jung, training=is_training)\n",
    "fc_jung = tf.nn.relu(fc_jung)\n",
    "fc_jung= tf.nn.dropout(fc_jung,keep_prob)\n",
    "\n",
    "fc_jong = tf.matmul(fc, W_jong1)\n",
    "fc_jong = tf.layers.batch_normalization(fc_jong, training=is_training)\n",
    "fc_jong = tf.nn.relu(fc_jong)\n",
    "fc_jong= tf.nn.dropout(fc_jong,keep_prob)\n",
    "print(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_3:0\", shape=(?, 19), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W_cho2 = tf.Variable(tf.random_normal([256,19],stddev=0.01))\n",
    "W_jung2 = tf.Variable(tf.random_normal([256,21],stddev=0.01))\n",
    "W_jong2 = tf.Variable(tf.random_normal([256,28],stddev=0.01))\n",
    "\n",
    "logit_cho = tf.matmul(fc_cho, W_cho2)\n",
    "logit_jung = tf.matmul(fc_jung, W_jung2)\n",
    "logit_jong = tf.matmul(fc_jong, W_jong2)\n",
    "\n",
    "print(logit_cho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "cross_entropy_cho = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit_cho, labels=input_label_cho)\n",
    "cross_entropy_jung = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit_jung, labels=input_label_jung)\n",
    "cross_entropy_jong = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit_jong, labels=input_label_jong)\n",
    "\n",
    "loss_cho = tf.reduce_mean(cross_entropy_cho)\n",
    "loss_jung = tf.reduce_mean(cross_entropy_jung)\n",
    "loss_jong = tf.reduce_mean(cross_entropy_jong)\n",
    "\n",
    "total_loss = loss_cho + loss_jung + loss_jong\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(0.0001).minimize(total_loss)\n",
    "loss_summ=tf.summary.scalar(\"Loss\",total_loss)\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ArgMax:0\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "argmax_cho = tf.argmax(logit_cho,1)\n",
    "argmax_jung = tf.argmax(logit_jung,1)\n",
    "argmax_jong = tf.argmax(logit_jong,1)\n",
    "print(argmax_cho)\n",
    "prediction = tf.stack([argmax_cho,argmax_jung,argmax_jong],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_labels = tf.stack([input_label_cho, input_label_jung, input_label_jong],1)\n",
    "equal_check = tf.equal(prediction,total_labels)\n",
    "cast_equal = tf.cast(equal_check, tf.float32)\n",
    "reduce_min = tf.reduce_min(cast_equal,1)\n",
    "\n",
    "accuracy = tf.reduce_mean(reduce_min)\n",
    "test_accuracy = tf.reduce_sum(reduce_min)\n",
    "acc_summ=tf.summary.scalar(\"Accuracy\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train total Batch :2002\n",
      "Epoch :0 | step 0, training accuracy: 0.000000, train loss: 9.326351\n",
      "Epoch :0 | step 100, training accuracy: 0.000000, train loss: 8.783644\n",
      "Epoch :0 | step 200, training accuracy: 0.000000, train loss: 7.485042\n",
      "Epoch :0 | step 300, training accuracy: 0.330000, train loss: 4.967968\n",
      "Epoch :0 | step 400, training accuracy: 0.480000, train loss: 3.432831\n",
      "Epoch :0 | step 500, training accuracy: 0.610000, train loss: 2.801615\n",
      "Epoch :0 | step 600, training accuracy: 0.510000, train loss: 2.567747\n",
      "Epoch :0 | step 700, training accuracy: 0.750000, train loss: 1.822445\n",
      "Epoch :0 | step 800, training accuracy: 0.620000, train loss: 1.926883\n",
      "Epoch :0 | step 900, training accuracy: 0.780000, train loss: 1.368045\n",
      "Epoch :0 | step 1000, training accuracy: 0.730000, train loss: 1.320316\n",
      "Epoch :0 | step 1100, training accuracy: 0.710000, train loss: 1.227501\n",
      "Epoch :0 | step 1200, training accuracy: 0.810000, train loss: 1.155586\n",
      "Epoch :0 | step 1300, training accuracy: 0.850000, train loss: 0.757622\n",
      "Epoch :0 | step 1400, training accuracy: 0.870000, train loss: 0.806618\n",
      "Epoch :0 | step 1500, training accuracy: 0.790000, train loss: 1.037640\n",
      "Epoch :0 | step 1600, training accuracy: 0.870000, train loss: 0.720959\n",
      "Epoch :0 | step 1700, training accuracy: 0.870000, train loss: 0.664788\n",
      "Epoch :0 | step 1800, training accuracy: 0.780000, train loss: 0.819824\n",
      "Epoch :0 | step 1900, training accuracy: 0.760000, train loss: 0.853213\n",
      "Epoch :0 | step 2000, training accuracy: 0.790000, train loss: 0.787943\n",
      "Time : 0 : 3: :3.878517\n",
      "Test Acc : 0.867869, Test Loss :0.594639\n",
      "Epoch :1 | step 0, training accuracy: 0.770000, train loss: 0.798594\n",
      "Epoch :1 | step 100, training accuracy: 0.870000, train loss: 0.532324\n",
      "Epoch :1 | step 200, training accuracy: 0.840000, train loss: 0.699049\n",
      "Epoch :1 | step 300, training accuracy: 0.810000, train loss: 0.770113\n",
      "Epoch :1 | step 400, training accuracy: 0.860000, train loss: 0.659908\n",
      "Epoch :1 | step 500, training accuracy: 0.830000, train loss: 0.596395\n",
      "Epoch :1 | step 600, training accuracy: 0.890000, train loss: 0.488906\n",
      "Epoch :1 | step 700, training accuracy: 0.870000, train loss: 0.471332\n",
      "Epoch :1 | step 800, training accuracy: 0.850000, train loss: 0.748979\n",
      "Epoch :1 | step 900, training accuracy: 0.860000, train loss: 0.541513\n",
      "Epoch :1 | step 1000, training accuracy: 0.750000, train loss: 0.725702\n",
      "Epoch :1 | step 1100, training accuracy: 0.890000, train loss: 0.521410\n",
      "Epoch :1 | step 1200, training accuracy: 0.880000, train loss: 0.377093\n",
      "Epoch :1 | step 1300, training accuracy: 0.890000, train loss: 0.464554\n",
      "Epoch :1 | step 1400, training accuracy: 0.870000, train loss: 0.469946\n",
      "Epoch :1 | step 1500, training accuracy: 0.910000, train loss: 0.324452\n",
      "Epoch :1 | step 1600, training accuracy: 0.820000, train loss: 0.554008\n",
      "Epoch :1 | step 1700, training accuracy: 0.870000, train loss: 0.512226\n",
      "Epoch :1 | step 1800, training accuracy: 0.870000, train loss: 0.517244\n",
      "Epoch :1 | step 1900, training accuracy: 0.910000, train loss: 0.573526\n",
      "Epoch :1 | step 2000, training accuracy: 0.960000, train loss: 0.234689\n",
      "Time : 0 : 6: :15.274464\n",
      "Test Acc : 0.914619, Test Loss :0.328426\n",
      "Epoch :2 | step 0, training accuracy: 0.870000, train loss: 0.425817\n",
      "Epoch :2 | step 100, training accuracy: 0.840000, train loss: 0.496659\n",
      "Epoch :2 | step 200, training accuracy: 0.910000, train loss: 0.522483\n",
      "Epoch :2 | step 300, training accuracy: 0.840000, train loss: 0.429381\n",
      "Epoch :2 | step 400, training accuracy: 0.860000, train loss: 0.390938\n",
      "Epoch :2 | step 500, training accuracy: 0.900000, train loss: 0.436640\n",
      "Epoch :2 | step 600, training accuracy: 0.930000, train loss: 0.252409\n",
      "Epoch :2 | step 700, training accuracy: 0.900000, train loss: 0.433330\n",
      "Epoch :2 | step 800, training accuracy: 0.900000, train loss: 0.423136\n",
      "Epoch :2 | step 900, training accuracy: 0.880000, train loss: 0.351092\n",
      "Epoch :2 | step 1000, training accuracy: 0.910000, train loss: 0.288153\n",
      "Epoch :2 | step 1100, training accuracy: 0.940000, train loss: 0.230172\n",
      "Epoch :2 | step 1200, training accuracy: 0.970000, train loss: 0.159228\n",
      "Epoch :2 | step 1300, training accuracy: 0.960000, train loss: 0.165451\n",
      "Epoch :2 | step 1400, training accuracy: 0.880000, train loss: 0.541893\n",
      "Epoch :2 | step 1500, training accuracy: 0.950000, train loss: 0.265671\n",
      "Epoch :2 | step 1600, training accuracy: 0.940000, train loss: 0.256895\n",
      "Epoch :2 | step 1700, training accuracy: 0.920000, train loss: 0.286367\n",
      "Epoch :2 | step 1800, training accuracy: 0.900000, train loss: 0.353842\n",
      "Epoch :2 | step 1900, training accuracy: 0.920000, train loss: 0.285624\n",
      "Epoch :2 | step 2000, training accuracy: 0.840000, train loss: 0.435216\n",
      "Time : 0 : 9: :26.804419\n",
      "Test Acc : 0.873638, Test Loss :0.434894\n",
      "Epoch :3 | step 0, training accuracy: 0.860000, train loss: 0.607469\n"
     ]
    }
   ],
   "source": [
    "init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print(\"Train total Batch :%d\" % (train_total_batch))\n",
    "with tf.Session() as sess:\n",
    "    merged_summary = tf.summary.merge([loss_summ, acc_summ])\n",
    "    train_writer = tf.summary.FileWriter(\"./logs/CNNV2/PE92/Train\")\n",
    "    test_writer = tf.summary.FileWriter(\"./logs/CNNV2/PE92/Test\")\n",
    "    \n",
    "    sess.run(init)\n",
    "    coord=tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    sess.run(train_init_op)\n",
    "    \n",
    "    #Train Model\n",
    "    \n",
    "    start_time = time.time()\n",
    "    global_step=0\n",
    "    for epoch in range(100):\n",
    "        for i in range(train_total_batch):\n",
    "        #for i in range(batch_size):\n",
    "            batch_images, batch_cho, batch_jung, batch_jong = sess.run([train_image, train_cho, train_jung, train_jong])\n",
    "            \n",
    "            sess.run(optimizer,feed_dict={input_images: batch_images, input_label_cho:batch_cho, \n",
    "                                                               input_label_jung : batch_jung, input_label_jong : batch_jong, keep_prob:0.5, is_training:True})\n",
    "            train_acc, train_loss=sess.run([accuracy, total_loss], feed_dict={input_images: batch_images, input_label_cho:batch_cho, \n",
    "                                                               input_label_jung : batch_jung, input_label_jong : batch_jong, keep_prob:1.0, is_training:False})\n",
    "            \n",
    "            if(i%100==0):\n",
    "                print('Epoch :%d | step %d, training accuracy: %f, train loss: %f' % (epoch,i, train_acc,train_loss))\n",
    "                summary = sess.run(merged_summary, feed_dict={input_images: batch_images, input_label_cho:batch_cho, input_label_jung : batch_jung, input_label_jong : batch_jong, keep_prob:1.0, is_training:False})\n",
    "                train_writer.add_summary(summary,global_step)\n",
    "                global_step+=1\n",
    "                \n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(\"Time : %d : %d: :%f\" %(int(elapsed/3600), int((elapsed%3600)/60), (elapsed%60)))\n",
    "        save_path = saver.save(sess, \"save/PE92/CNNV2/CNNV2_PE92.ckpt\")\n",
    "        \n",
    "        acc_sum=0.0\n",
    "        loss_sum=0.\n",
    "        sess.run(test_init_op)\n",
    "        for epoch in range(total_test_batch):\n",
    "            batch_images, batch_cho, batch_jung, batch_jong = sess.run([test_image, test_cho, test_jung, test_jong])\n",
    "            try:\n",
    "                test_acc, test_loss=sess.run([test_accuracy,total_loss], feed_dict={input_images: batch_images, input_label_cho:batch_cho, input_label_jung : batch_jung, input_label_jong : batch_jong, keep_prob:1.0, is_training:False})\n",
    "            \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Out of Data ierators\")\n",
    "            acc_sum+=test_acc\n",
    "            loss_sum+=test_loss*len(batch_cho)\n",
    "        test_loss=loss_sum/test_data_length\n",
    "        test_acc = acc_sum/test_data_length\n",
    "        print(\"Test Acc : %f, Test Loss :%f\" %(test_acc,test_loss))\n",
    "    \n",
    "    print(\"Training is done.\")\n",
    "    sess.run(test_init_op)\n",
    "    acc_sum=0.0\n",
    "    loss_sum=0.0\n",
    "    for epoch in range(total_test_batch):\n",
    "        batch_images, batch_cho, batch_jung, batch_jong = sess.run([test_image, test_cho, test_jung, test_jong])\n",
    "        try:\n",
    "            test_acc, test_loss=sess.run([test_accuracy,total_loss], feed_dict={input_images: batch_images, input_label_cho:batch_cho, input_label_jung : batch_jung, input_label_jong : batch_jong, keep_prob:1.0, is_training:False})\n",
    "            loss_sum+=test_loss*len(batch_cho)\n",
    "        \n",
    "        except tf.errors.OutOfRangeError:\n",
    "              print(\"Out of Data ierators\")\n",
    "        acc_sum+=test_acc\n",
    "    \n",
    "    test_loss=loss_sum/test_data_length\n",
    "    test_acc = acc_sum/test_data_length\n",
    "    print(\"Test Acc : %f, Test Loss :%f\" %(test_acc,test_loss))\n",
    "\n",
    "\n",
    "    train_writer.close()\n",
    "    test_writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = \"Shuffled_with_alphabet_HanDB_Train.tfrecord\"\n",
    "test_file_name = \"Shuffled_with_alphabet_HanDB_Test.tfrecord\"\n",
    "\n",
    "train_data_length = sum(1 for _ in tf.python_io.tf_record_iterator(\"./\"+train_file_name))\n",
    "test_data_length = sum(1 for _ in tf.python_io.tf_record_iterator(\"./\"+test_file_name))\n",
    "total_test_batch = int(test_data_length/batch_size)+1\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(\"./\"+train_file_name)\n",
    "train_dataset = train_dataset.map(decode)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset= train_dataset.shuffle(buffer_size=10000)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset(\"./\"+test_file_name)\n",
    "test_dataset = test_dataset.map(decode)\n",
    "#test_dataset = test_dataset.repeat()\n",
    "#test_dataset= test_dataset.shuffle(buffer_size=10000)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "train_iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "test_iterator = tf.data.Iterator.from_structure(test_dataset.output_types,\n",
    "                                           test_dataset.output_shapes)\n",
    "\n",
    "train_init_op = train_iterator.make_initializer(train_dataset)\n",
    "test_init_op = test_iterator.make_initializer(test_dataset)\n",
    "\n",
    "train_image, train_label, train_cho, train_jung, train_jong = train_iterator.get_next()\n",
    "test_image, test_label, test_cho, test_jung, test_jong = test_iterator.get_next()\n",
    "\n",
    "train_total_batch = int(train_data_length/batch_size)\n",
    "test_total_batch = int(test_data_length/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print(\"Train total Batch :%d\" % (train_total_batch))\n",
    "with tf.Session() as sess:\n",
    "    merged_summary3 = tf.summary.merge([loss_summ, acc_summ])\n",
    "    train_writer = tf.summary.FileWriter(\"./logs/CNNV2/HanDB/Train\")\n",
    "    test_writer = tf.summary.FileWriter(\"./logs/CNNV2/HanDB/Test\")\n",
    "    \n",
    "    sess.run(init)\n",
    "    coord=tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    sess.run(train_init_op)\n",
    "   \n",
    "    #Train Model\n",
    "    \n",
    "    start_time = time.time()\n",
    "    global_step=0\n",
    "    for epoch in range(100):\n",
    "        for i in range(train_total_batch):\n",
    "        #for i in range(batch_size):\n",
    "            batch_images, batch_cho, batch_jung, batch_jong = sess.run([train_image, train_cho, train_jung, train_jong])\n",
    "            \n",
    "            sess.run(optimizer,feed_dict={input_images: batch_images, input_label_cho:batch_cho, \n",
    "                                                               input_label_jung : batch_jung, input_label_jong : batch_jong, keep_prob:0.5, is_training:True})\n",
    "            train_acc, train_loss=sess.run([accuracy, total_loss], feed_dict={input_images: batch_images, input_label_cho:batch_cho, \n",
    "                                                               input_label_jung : batch_jung, input_label_jong : batch_jong, keep_prob:1.0, is_training:False})\n",
    "            \n",
    "            if(i%100==0):\n",
    "                print('Epoch :%d | step %d, training accuracy: %f, train loss: %f' % (epoch,i, train_acc,train_loss))\n",
    "                summary = sess.run(merged_summary, feed_dict={input_images: batch_images, input_label_cho:batch_cho, input_label_jung : batch_jung, input_label_jong : batch_jong, keep_prob:1.0, is_training:False})\n",
    "                train_writer.add_summary(summary,global_step)\n",
    "                global_step+=1\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(\"Time : %d : %d: :%f\" %(int(elapsed/3600), int((elapsed%3600)/60), (elapsed%60)))\n",
    "        save_path = saver.save(sess, \"save/HanDB/CNNV2/CNNV2_HanDB.ckpt\")\n",
    "        \n",
    "        acc_sum=0.0\n",
    "        loss_sum=0.0\n",
    "        sess.run(test_init_op)\n",
    "        for epoch in range(total_test_batch):\n",
    "            batch_images, batch_cho, batch_jung, batch_jong = sess.run([test_image, test_cho, test_jung, test_jong])\n",
    "            try:\n",
    "                test_acc, test_loss=sess.run([test_accuracy,total_loss], feed_dict={input_images: batch_images, input_label_cho:batch_cho, input_label_jung : batch_jung, input_label_jong : batch_jong, keep_prob:1.0, is_training:False})\n",
    "                acc=test_acc/len(batch_cho)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Out of Data ierators\")\n",
    "            acc_sum+=test_acc\n",
    "            loss_sum+=test_loss*len(batch_cho)\n",
    "        test_loss =loss_sum/test_data_length\n",
    "        test_acc = acc_sum/test_data_length\n",
    "        print(\"Test Acc : %f, Test Loss :%f\" %(test_acc,test_loss))\n",
    "        \n",
    "    \n",
    "    print(\"Training is done.\")\n",
    "    acc_sum=0.0\n",
    "    loss_sum=0.0\n",
    "    sess.run(test_init_op)\n",
    "    for epoch in range(total_test_batch):\n",
    "        batch_images, batch_cho, batch_jung, batch_jong = sess.run([test_image, test_cho, test_jung, test_jong])\n",
    "        try:\n",
    "            test_acc, test_loss=sess.run([test_accuracy,total_loss], feed_dict={input_images: batch_images, input_label_cho:batch_cho, input_label_jung : batch_jung, input_label_jong : batch_jong, keep_prob:1.0, is_training:False})\n",
    "            test_acc=test_acc/len(batch_cho)\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "              print(\"Out of Data ierators\")\n",
    "        acc_sum+=test_acc\n",
    "        loss_sum+=test_loss*len(batch_cho)\n",
    "    test_loss=loss_sum/test_data_length\n",
    "    test_acc = acc_sum/test_data_length\n",
    "    print(\"Test Acc : %f, Test Loss :%f\" %(test_acc,test_loss))\n",
    "\n",
    "\n",
    "    #Test Model\n",
    "    train_writer.close()\n",
    "    test_writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2_cho2 = tf.Variable(tf.random_normal([256,19],stddev=0.01))\n",
    "W2_jung2 = tf.Variable(tf.random_normal([256,21],stddev=0.01))\n",
    "W2_jong2 = tf.Variable(tf.random_normal([256,28],stddev=0.01))\n",
    "\n",
    "fc2_cho2 = tf.matmul(fc_cho, W_cho2)\n",
    "bn2_cho2 = tf.layers.batch_normalization(fc2_cho2, training=is_training)\n",
    "fc2_cho2 = tf.nn.relu(bn2_cho2)\n",
    "fc2_cho2 = tf.nn.dropout(fc2_cho2, keep_prob)\n",
    "\n",
    "fc2_jung2 = tf.matmul(fc_jung, W_jung2)\n",
    "bn2_jung2 = tf.layers.batch_normalization(fc2_jung2, training=is_training)\n",
    "fc2_jung2 = tf.nn.relu(bn2_jung2)\n",
    "fc2_jung2 = tf.nn.dropout(fc2_jung2, keep_prob)\n",
    "\n",
    "fc2_jong2 = tf.matmul(fc_jong, W_jong2)\n",
    "bn2_jong2 = tf.layers.batch_normalization(fc2_jong2, training=is_training)\n",
    "fc2_jong2 = tf.nn.relu(bn2_jong2)\n",
    "fc2_jong2 = tf.nn.dropout(fc2_jong2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_cho3 = tf.Variable(tf.random_normal([19,128],stddev=0.01))\n",
    "W_jung3 = tf.Variable(tf.random_normal([21,128],stddev=0.01))\n",
    "W_jong3 = tf.Variable(tf.random_normal([28,128],stddev=0.01))\n",
    "\n",
    "fc3_cho2 = tf.matmul(fc2_cho2, W_cho3)\n",
    "fc3_jung2 = tf.matmul(fc2_jung2, W_jung3)\n",
    "fc3_jong2 = tf.matmul(fc2_jong2, W_jong3)\n",
    "\n",
    "fc_total2 = fc3_cho2 + fc3_jung2 + fc3_jong2\n",
    "bn_total2 = tf.layers.batch_normalization(fc_total2, training=is_training)\n",
    "fc_total2 = tf.nn.relu(bn_total2)\n",
    "fc_total2 = tf.nn.dropout(fc_total2, keep_prob)\n",
    "\n",
    "W_total = tf.Variable(tf.random_normal([128,2350], stddev=0.01))\n",
    "logit = tf.matmul(fc_total2, W_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy2 = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=input_labels)\n",
    "loss = tf.reduce_mean(cross_entropy2)\n",
    "update_ops2 = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops2):\n",
    "    optimizer2 = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "loss_summ2=tf.summary.scalar(\"Loss\",loss)\n",
    "        \n",
    "correct_prediction2 = tf.equal(tf.argmax(logit,1), input_labels)\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, tf.float32))\n",
    "test_accuracy2 = tf.reduce_sum(tf.cast(correct_prediction2, tf.float32))\n",
    "acc_summ2=tf.summary.scalar(\"Accuracy\",accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode2(serialized_example):\n",
    "    features = tf.parse_single_example(\n",
    "      serialized_example,      \n",
    "      features={\n",
    "          'label': tf.FixedLenFeature([], tf.int64),\n",
    "          'width': tf.FixedLenFeature([], tf.int64),\n",
    "          'height': tf.FixedLenFeature([], tf.int64),\n",
    "          'image': tf.FixedLenFeature([], tf.string)\n",
    "      })\n",
    "    image = tf.decode_raw(features['image'],tf.uint8)\n",
    "    #image = tf.reshape(image,[64,64,1])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [64,64,1])\n",
    "    label = tf.cast(features['label'],tf.int64)\n",
    "    \n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "test_file_name = \"Shuffled_PE92_Test.tfrecord\"\n",
    "train_file_name = \"Shuffled_PE92_Train.tfrecord\"\n",
    "\n",
    "train_data_length = sum(1 for _ in tf.python_io.tf_record_iterator(\"./\"+train_file_name))\n",
    "test_data_length = sum(1 for _ in tf.python_io.tf_record_iterator(\"./\"+test_file_name))\n",
    "total_test_batch = int(test_data_length/batch_size)+1\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset(\"./\"+test_file_name)\n",
    "test_dataset = test_dataset.map(decode2)\n",
    "#test_dataset = test_dataset.repeat()\n",
    "#test_dataset= test_dataset.shuffle(buffer_size=10000)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(\"./\"+train_file_name)\n",
    "train_dataset = train_dataset.map(decode2)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset= train_dataset.shuffle(buffer_size=10000)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "train_iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "test_iterator = tf.data.Iterator.from_structure(test_dataset.output_types,\n",
    "                                           test_dataset.output_shapes)\n",
    "\n",
    "train_init_op = train_iterator.make_initializer(train_dataset)\n",
    "test_init_op = test_iterator.make_initializer(test_dataset)\n",
    "\n",
    "train_image, train_label = train_iterator.get_next()\n",
    "test_image, test_label = test_iterator.get_next()\n",
    "\n",
    "train_total_batch = int(train_data_length/batch_size)\n",
    "test_total_batch = int(test_data_length/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print(\"Train total Batch :%d\" % (train_total_batch))\n",
    "with tf.Session() as sess:\n",
    "    merged_summary3 = tf.summary.merge([loss_summ2, acc_summ2])\n",
    "    train_writer = tf.summary.FileWriter(\"./logs/CNNV3/PE92/Train\")\n",
    "    test_writer = tf.summary.FileWriter(\"./logs/CNNV3/PE92/Test\")\n",
    "    \n",
    "    sess.run(init)\n",
    "    coord=tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    sess.run(train_init_op)\n",
    "    sess.run(test_init_op)\n",
    "    #Train Model\n",
    "    \n",
    "    start_time = time.time()\n",
    "    global_step=0\n",
    "    for epoch in range(100):\n",
    "        for i in range(train_total_batch):\n",
    "        #for i in range(batch_size):\n",
    "            batch_images, batch_labels = sess.run([train_image, train_label])\n",
    "            \n",
    "            sess.run(optimizer2,feed_dict={input_images: batch_images, input_labels: batch_labels, keep_prob:0.5, is_training:True})\n",
    "            train_acc, train_loss=sess.run([accuracy2, loss], feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:1.0, is_training:False})\n",
    "            \n",
    "            if(i%100==0):\n",
    "                print('Epoch :%d | step %d, training accuracy: %f, train loss: %f' % (epoch,i, train_acc,train_loss))\n",
    "                summary = sess.run(merged_summary3, feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:1.0, is_training:False})\n",
    "                train_writer.add_summary(summary,global_step)\n",
    "                global_step+=1\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(\"Time : %d : %d: :%f\" %(int(elapsed/3600), int((elapsed%3600)/60), (elapsed%60)))\n",
    "        save_path = saver.save(sess, \"save/PE92/CNNV3/CNNV3_PE92.ckpt\")\n",
    "        acc_sum=0.0\n",
    "        loss_sum=0.0\n",
    "        sess.run(test_init_op)\n",
    "        for epoch in range(total_test_batch):\n",
    "            batch_images, batch_labels = sess.run([test_image, test_label])\n",
    "            try:\n",
    "                test_acc, test_loss=sess.run([test_accuracy2, loss], feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:1.0, is_training:False})\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Out of Data ierators\")\n",
    "            acc_sum+=test_acc\n",
    "            loss_sum+=test_loss*len(batch_cho)\n",
    "        loss_sum=loss_sum/test_data_length\n",
    "        test_acc = acc_sum/test_data_length\n",
    "        print(\"Test Acc : %f, Test Loss :%f\" %(test_acc,test_loss))\n",
    "    \n",
    "    print(\"Training is done.\")\n",
    "    acc_sum=0.0\n",
    "    loss_sum=0.0\n",
    "    sess.run(test_init_op)\n",
    "    for epoch in range(total_test_batch):\n",
    "        batch_images, batch_labels = sess.run([test_image, test_label])\n",
    "        try:\n",
    "            test_acc, test_loss=sess.run([test_accuracy2, loss], feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:1.0, is_training:False})\n",
    "        except tf.errors.OutOfRangeError:\n",
    "              print(\"Out of Data ierators\")\n",
    "        acc_sum+=test_acc\n",
    "        loss_sum+=test_loss*len(batch_cho)\n",
    "    test_loss=loss_sum/test_data_length\n",
    "    test_acc = acc_sum/test_data_length\n",
    "    print(\"Test Acc : %f, Test Loss :%f\" %(test_acc,test_loss))\n",
    "\n",
    "\n",
    "    train_writer.close()\n",
    "    test_writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = \"Shuffled_HanDB_Train.tfrecord\"\n",
    "test_file_name = \"Shuffled_HanDB_Test.tfrecord\"\n",
    "\n",
    "train_data_length = sum(1 for _ in tf.python_io.tf_record_iterator(\"./\"+train_file_name))\n",
    "test_data_length = sum(1 for _ in tf.python_io.tf_record_iterator(\"./\"+test_file_name))\n",
    "total_test_batch = int(test_data_length/batch_size)+1\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(\"./\"+train_file_name)\n",
    "train_dataset = train_dataset.map(decode2)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset= train_dataset.shuffle(buffer_size=10000)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset(\"./\"+test_file_name)\n",
    "test_dataset = test_dataset.map(decode2)\n",
    "#test_dataset = test_dataset.repeat()\n",
    "#test_dataset= test_dataset.shuffle(buffer_size=10000)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "train_iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "test_iterator = tf.data.Iterator.from_structure(test_dataset.output_types,\n",
    "                                           test_dataset.output_shapes)\n",
    "\n",
    "train_init_op = train_iterator.make_initializer(train_dataset)\n",
    "test_init_op = test_iterator.make_initializer(test_dataset)\n",
    "\n",
    "train_image, train_label = train_iterator.get_next()\n",
    "test_image, test_label = test_iterator.get_next()\n",
    "\n",
    "train_total_batch = int(train_data_length/batch_size)\n",
    "test_total_batch = int(test_data_length/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print(\"Train total Batch :%d\" % (train_total_batch))\n",
    "with tf.Session() as sess:\n",
    "    merged_summary4 = tf.summary.merge([loss_summ2, acc_summ2])\n",
    "    train_writer = tf.summary.FileWriter(\"./logs/CNNV3/HanDB/Train\")\n",
    "    test_writer = tf.summary.FileWriter(\"./logs/CNNV3/HanDB/Test\")\n",
    "    \n",
    "    sess.run(init)\n",
    "    coord=tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    sess.run(train_init_op)\n",
    "\n",
    "    #Train Model\n",
    "    \n",
    "    start_time = time.time()\n",
    "    global_step=0\n",
    "    for epoch in range(100):\n",
    "        for i in range(train_total_batch):\n",
    "        #for i in range(batch_size):\n",
    "            batch_images, batch_labels = sess.run([train_image, train_label])\n",
    "            \n",
    "            sess.run(optimizer2,feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:0.5, is_training:True})\n",
    "            train_acc, train_loss=sess.run([accuracy2, loss], feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:1.0, is_training:False})\n",
    "            \n",
    "            if(i%100==0):\n",
    "                print('Epoch :%d | step %d, training accuracy: %f, train loss: %f' % (epoch,i, train_acc,train_loss))\n",
    "                summary = sess.run(merged_summary4, feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:1.0, is_training:False})\n",
    "                train_writer.add_summary(summary,global_step)\n",
    "                global_step+=1\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(\"Time : %d : %d: :%f\" %(int(elapsed/3600), int((elapsed%3600)/60), (elapsed%60)))\n",
    "        save_path = saver.save(sess, \"save/HanDB/CNNV3/CNNV3_PE92.ckpt\")\n",
    "        \n",
    "        acc_sum=0.0\n",
    "        loss_sum=0.0\n",
    "        sess.run(test_init_op)\n",
    "        for epoch in range(total_test_batch):\n",
    "            batch_images, batch_labels = sess.run([test_image, test_label])\n",
    "            try:\n",
    "                test_acc, test_loss=sess.run([test_accuracy2, loss], feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:1.0, is_training:False})\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Out of Data ierators\")\n",
    "            acc_sum+=test_acc\n",
    "            loss_sum+=test_loss*len(batch_cho)\n",
    "        loss_sum=loss_sum/test_data_length\n",
    "        test_acc = acc_sum/test_data_length\n",
    "        print(\"Test Acc : %f, Test Loss :%f\" %(test_acc,test_loss))\n",
    "    \n",
    "    print(\"Training is done.\")\n",
    "    acc_sum=0.0\n",
    "    loss_sum=0.0\n",
    "    sess.run(test_init_op)\n",
    "    for epoch in range(total_test_batch):\n",
    "        batch_images, batch_labels = sess.run([test_image, test_label])\n",
    "        try:\n",
    "            test_acc, test_loss=sess.run([test_accuracy2, loss], feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:1.0, is_training:False})\n",
    "        except tf.errors.OutOfRangeError:\n",
    "              print(\"Out of Data ierators\")\n",
    "        acc_sum+=test_acc\n",
    "        loss_sum+=test_loss*len(batch_cho)\n",
    "    test_loss=loss_sum/test_data_length\n",
    "    test_acc = acc_sum/test_data_length\n",
    "    print(\"Test Acc : %f, Test Loss :%f\" %(test_acc,test_loss))\n",
    "\n",
    "\n",
    "    #Test Model\n",
    "    train_writer.close()\n",
    "    test_writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
