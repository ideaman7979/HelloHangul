{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(serialized_example):\n",
    "    features = tf.parse_single_example(\n",
    "      serialized_example,      \n",
    "      features={\n",
    "          'label': tf.FixedLenFeature([], tf.int64),\n",
    "          'width': tf.FixedLenFeature([], tf.int64),\n",
    "          'height': tf.FixedLenFeature([], tf.int64),\n",
    "          'image': tf.FixedLenFeature([], tf.string)\n",
    "      })\n",
    "    image = tf.decode_raw(features['image'],tf.uint8)\n",
    "    #image = tf.reshape(image,[64,64,1])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [64,64,1])\n",
    "    label = tf.cast(features['label'],tf.int64)\n",
    "    \n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "test_file_name = \"index_HanDB_Test.tfrecord\"\n",
    "test_dataset = tf.data.TFRecordDataset(\"./\"+test_file_name)\n",
    "test_dataset = test_dataset.map(decode)\n",
    "test_dataset = test_dataset.repeat()\n",
    "test_dataset= test_dataset.shuffle(buffer_size=10000)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_length = sum(1 for _ in tf.python_io.tf_record_iterator(\"./\"+test_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.data.Iterator.from_structure(test_dataset.output_types,\n",
    "                                           test_dataset.output_shapes)\n",
    "next_image, next_label = iterator.get_next()\n",
    "\n",
    "test_init_op = iterator.make_initializer(test_dataset)\n",
    "\n",
    "test_total_batch = int(test_data_length/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 64, 64, 1), dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "input_images = tf.placeholder(tf.float32,[None,64,64,1])\n",
    "input_labels = tf.placeholder(tf.int64,[None])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "print(input_images)\n",
    "print(input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D:0\", shape=(?, 64, 64, 32), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 32, 32, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W1 = tf.Variable(tf.random_normal([5,5,1,32], stddev=0.01))\n",
    "L1 = tf.nn.conv2d(input_images,W1, strides=[1,1,1,1], padding = 'SAME')\n",
    "print(L1)\n",
    "bn1 = tf.layers.batch_normalization(L1, training=True)\n",
    "at1 = tf.nn.relu(bn1)\n",
    "\n",
    "pool1 =  tf.nn.max_pool(at1, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "print(pool1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_2:0\", shape=(?, 16, 16, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W2 = tf.Variable(tf.random_normal([5, 5, 32, 64], stddev=0.01))\n",
    "\n",
    "L2 = tf.nn.conv2d(pool1,W2, strides=[1,1,1,1], padding = 'SAME')\n",
    "bn2 = tf.layers.batch_normalization(L2, training=True)\n",
    "at2 = tf.nn.relu(bn2)\n",
    "\n",
    "pool2 =  tf.nn.max_pool(at2, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "print(pool2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_3:0\", shape=(?, 8, 8, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W3 = tf.Variable(tf.random_normal([4, 4, 64, 128], stddev=0.01))\n",
    "\n",
    "L3 = tf.nn.conv2d(pool2,W3, strides=[1,1,1,1], padding = 'SAME')\n",
    "bn3 = tf.layers.batch_normalization(L3, training=True)\n",
    "at3 = tf.nn.relu(bn3)\n",
    "\n",
    "pool3 =  tf.nn.max_pool(at3, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "print(pool3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_4:0\", shape=(?, 4, 4, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W4 = tf.Variable(tf.random_normal([4, 4, 128, 256], stddev=0.01))\n",
    "\n",
    "L4 = tf.nn.conv2d(pool3,W4, strides=[1,1,1,1], padding = 'SAME')\n",
    "bn4 = tf.layers.batch_normalization(L4, training=True)\n",
    "at4 = tf.nn.relu(bn4)\n",
    "\n",
    "pool4 =  tf.nn.max_pool(at4, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "print(pool4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dropout/mul:0\", shape=(?, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W5 = tf.Variable(tf.random_normal([4*4*256, 1024], stddev=0.01))\n",
    "fc1 = tf.reshape(pool4,[-1,4*4*256])\n",
    "fc1 = tf.matmul(fc1, W5)\n",
    "fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "fc1 = tf.nn.dropout(fc1,keep_prob)\n",
    "print(fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_1:0\", shape=(?, 2350), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W6 = tf.Variable(tf.random_normal([1024,2350],stddev=0.01))\n",
    "logit = tf.matmul(fc1, W6)\n",
    "print(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=input_labels))\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ArgMax:0\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "label_max=tf.argmax(logit,1)\n",
    "print(label_max)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logit,1),input_labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File ./save\\model2.ckpt does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-88defaa8d9cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#total_batch = int(numb_of_data / batch_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#images, labels = read_and_decode(tf_record_filename_queue,batch_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./save\\model2.ckpt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[1;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[0;32m   1829\u001b[0m                        \"execution is enabled.\")\n\u001b[0;32m   1830\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1831\u001b[1;33m     \u001b[0mmeta_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1832\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1833\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    553\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File %s does not exist.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m   \u001b[1;31m# First try to read it as a binary file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: File ./save\\model2.ckpt does not exist."
     ]
    }
   ],
   "source": [
    "#total_batch = int(numb_of_data / batch_size)\n",
    "#images, labels = read_and_decode(tf_record_filename_queue,batch_size)\n",
    "saver = tf.train.import_meta_graph(\"./save\\model2.ckpt\")\n",
    "init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    coord=tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    sess.run(train_init_op)#Test iterator 생성\n",
    "    for i in range(test_total_batch):\n",
    "        batch_images, batch_labels = sess.run([next_image, next_label])\n",
    "        test_acc=sess.run(accuracy, feed_dict={input_images: batch_images, input_labels:batch_labels, keep_prob:1.0})\n",
    "        if(i%10==0):\n",
    "            print(\"Test Acc : \" %(test_acc))\n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
